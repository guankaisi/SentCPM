# Sentence Embedding for CPM-Bee

## ğŸ“–ä»‹ç»

æœ¬ä»“åº“é’ˆå¯¹[CPM-Bee](https://github.com/OpenBMB/CPM-Bee)ç”Ÿæˆå¥å­å‘é‡è¿›è¡Œäº†ç›¸åº”çš„å¢é‡å¾®è°ƒï¼Œå¹¶å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚

æˆ‘ä»¬ä¸»è¦ç”¨SimCSEä¸­æœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ å¯¹[CPM-Bee](https://github.com/OpenBMB/CPM-Bee)è¿›è¡Œå¢é‡å¾®è°ƒï¼Œå¹¶åœ¨[SentEval](https://github.com/facebookresearch/SentEval) ä¸Šæµ‹è¯•äº†æµ‹è¯•äº†CPM-Beeç”Ÿæˆçš„sentence Embeddingæ•ˆæœ, åŒæ—¶ä¸ºäº†æµ‹è¯•ä¸­æ–‡æ•ˆæœï¼Œæˆ‘ä»¬åœ¨SentEvalçš„åŸºç¡€ä¸Šæ·»åŠ äº†[Chinese STSBenchmark](https://github.com/pluto-junzeng/CNSD)æµ‹è¯•æ•°æ®ï¼Œå¹¶å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœ

## ğŸš€ å¦‚ä½•ä½¿ç”¨

1. å…‹éš†ä»“åº“å¹¶è¿›å…¥æºä»£ç 

```
git clone xxx/SentCPM
```

2.åˆ›å»ºconda ç¯å¢ƒ

```shell
conda create -n sentCPM python=3.10 -y
conda activate sentCPM
```

3.å®‰è£…ä¾èµ–

```shell
pip install torch>=1.10
pip install -r requirements.txt
```

4.ä¸‹è½½æ¨¡å‹å’Œdeltaæƒé‡åˆ°å¯¹åº”ä½ç½®

5.å‚è€ƒ`example.py`

åŠ è½½æ¨¡å‹å’Œdeltaæ¨¡å‹ï¼Œå°†éšè—å±‚å……å½“å…¶sentence-embedding

```python
bmt.init_distributed(seed=1024)
    # Load transformers' model checkpoint
    config = CPMBeeConfig.from_json_file('config/cpm-bee-1b.json')
    tokenizer = CPMBeeTokenizer()
    model = CPMBee(config=config)
    bmt.load(model,'cpm-bee-1b-ckpt.pt')
    delta_model = LoraModel(
            backbone_model=model, modified_modules=["project_q", "project_v"], backend="bmt"
    )
    delta_model.freeze_module(exclude=["deltas"], set_state_dict=True)
    delta_state = torch.load('cpm_finetune/cpm-bee-1b-delta.pt')
    model.load_state_dict(delta_state,strict=False)
    model.cuda()
    sentences = [
        'an impenetrable and insufferable ball of pseudo-philosophic twaddle .'
    ]
    # æ„å»ºæˆCPM-Beeçš„è¾“å…¥æ ¼å¼
    sentences = [sentence.replace('<','[').replace('>',']') for sentence in sentences]
    sentences_after = [{"input":sentence,"<ans>":""} for sentence in sentences]
    model_inputs, other_info = process_sentence_list(data_list=sentences_after)
    # æ„å»ºCPM-Beeè¾“å…¥æ ¼å¼
    with torch.no_grad():
        _, hidden_state = model(**model_inputs)
    sentence_embedding = torch.mean(hidden_state,dim=1)
    sentence_embedding = F.normalize(sentence_embedding,p=2,dim=1)
    if sentence_embedding.shape[0]>1:
        sentence_embedding = whitening(sentence_embedding)
    print("sentece: ",sentences)
    print("sentence_embedding: ",sentence_embedding)
```



## ğŸ‘€å¯¹CPMè¿›è¡Œå¢é‡å¾®è°ƒ

å¢é‡å¾®è°ƒåŸºäºOpendeltaï¼Œä½¿ç”¨äº†Loraæ–¹æ³•è¿›è¡Œå¾®è°ƒ

å‚è€ƒ[SimCSE](https://github.com/princeton-nlp/SimCSE)ä¸­æœ‰ç›‘ç£å¾®è°ƒçš„æ–¹å¼

è®­ç»ƒæ•°æ®é›†ä¸ºNLIæ•°æ®ï¼ŒåŒ…æ‹¬SNLIå’ŒMNLIæ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹è„šæœ¬ä¸‹è½½

```shell
cd data
bash download_nli.sh
```

å…·ä½“å¾®è°ƒä»£ç ï¼ŒæŒ‰ç…§[CPM-Beeå¾®è°ƒæ•™ç¨‹](https://github.com/OpenBMB/CPM-Bee/tree/main/tutorials/basic_task_finetune)æ”¹å†™

å‚è§`finetune.py`

è¿è¡Œè„šæœ¬ä¸º

```shell
#!/bin/bash
CUDA_VISIBLE_DEVICES= 0,7
torchrun --nnodes=1 --nproc_per_node=2 --rdzv_id=1  --rdzv_endpoint=localhost:12345 finetune_cpm.py \
--model-config /run/user/guankaisi/config/cpm-bee-1b.json \
--load /run/user/guankaisi/cpm-bee-1b-ckpt.pt \
--dataset data/nli_for_simcse.csv \
--epoch 2 \
--batch-size 16 \
--lr 1e-5 \
--save cpm_finetune/ \
--save-name cpm-bee-1b \
--use-delta
```



## ğŸ”—delta æ¨¡å‹

æˆ‘ä»¬å¯¹cpm-1bå’Œcpm-10bæ¨¡å‹è¿›è¡Œå¥å‘é‡å¾®è°ƒï¼Œå¹¶å¼€æºå‡ºç›¸åº”delta-model

| Model             | åŸºåº§æ¨¡å‹ | é“¾æ¥ |
| ----------------- | -------- | ---- |
| SentCPM-delta-1b  | CPM-1b   |      |
| SentCPM-delta-10b | CPM-10b  |      |

##  ğŸŒ¸å¦‚ä½•æµ‹è¯•

æ‰€æœ‰æµ‹è¯•æ•°æ®æˆ‘ä»¬éƒ½åŸºäº[SentEval](https://github.com/facebookresearch/SentEval)ï¼ŒåŒæ—¶æˆ‘ä»¬å‘åŸºç¡€çš„SentEvalç‰ˆæœ¬ä¸­æ·»åŠ äº†æµ‹è¯•ä¸­æ–‡èƒ½åŠ›çš„æ•°æ®é›†[CSTS-B](Chinese STSBenchmark), å¯ä»¥ä½¿ç”¨åŒä¸€å¥—æµ‹è¯•è„šæœ¬è¿›è¡Œæµ‹è¯•

æˆ‘ä»¬æœ‰ä¸¤ç§ç±»å‹çš„æµ‹è¯•ä»£ç ï¼Œä¸€ç§æ˜¯æµ‹è¯•åŸºäºHuggingfaceçš„æ¨¡å‹`evaluation.py`

æµ‹è¯•è„šæœ¬ç¤ºä¾‹ä¸º

```shell
#ï¼/bin/bash
CUDA_VISIBLE_DEVICES=3,4,5,6,7 torchrun --nnodes=1 --nproc_per_node=1 --rdzv_id=1 --rdzv_endpoint=localhost:12349 evaluation.py \
--model_name_or_path bert-base-uncased \
--pooler avg \
--task_set full \
--mode test \
```

å¦ä¸€ç§æ˜¯ä¸“é—¨æµ‹è¯•CPMæ¨¡å‹çš„æµ‹è¯•ç¨‹åº`evaluation_cpm.py`

æµ‹è¯•è„šæœ¬ç¤ºä¾‹ä¸º

```Shell
#!/bin/bash
python evaluation_cpm.py \
--model_name cpm-bee-1b-ckpt.pt \
--config_name config/cpm-bee-1b.json \
--delta_name cpm_finetune/cpm-bee-1b-delta.pt \
--pooler avg \
--task_set full \
--mode test \
```



## ğŸ’« æ€§èƒ½è¡¨ç°

æˆ‘ä»¬ä¸»è¦æµ‹è¯•äº†[SentEval](https://github.com/facebookresearch/SentEval)ä¸­çš„å„ä¸ªæ•°æ®é›†ï¼Œä»¥åŠä¸­æ–‡èƒ½åŠ›çš„Chinese STS-B

æˆ‘ä»¬é¦–å…ˆåœ¨è¯­å¥ç›¸ä¼¼åº¦åŒ¹é…STSç³»åˆ—æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†å¤§äºGPT-ada-embeddingçš„æ•ˆæœ

| Model                                               | STS12     | STS13     | STS14     | STS15     | STS16     | STSBenchmark | SICKRelatedness | Avg        |
| --------------------------------------------------- | --------- | --------- | --------- | --------- | --------- | ------------ | --------------- | ---------- |
| **simcse-CPM-1b**                                   | 75.24     | 84.81     | 79.85     | 82.29     | 79.99     | 83.85        | 77.42           | 80.49      |
| **simcse-CPM-10b**                                  | 76.84     | **87.37** | **82.21** | 82.78     | 82.66     | 86.48        | 79.79           | **82.59**  |
| GPT-embedding-002                                   | 71.08     | 81.85     | 76.37     | **86.03** | **85.60** | 84.30        | 80.25           | 80.78      |
| sup-simcse-robert                                   | **78.81** | 86.37     | 81.35     | 84.27     | 81.52     | **86.63  **  | **81.39  **     | **82.91 ** |
| sup-simcse-bert                                     | 75.17     | 82.73     | 77.52     | 85.74     | 80.84     | 82.95        | 80.56           | 80.79      |
| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 69.00     | 70.92     | 71.42     | 83.12     | 76.80     | 77.52        | 66.57           | 73.62      |
| bert-avg-whitening                                  | 30.87     | 59.89     | 47.73     | 60.29     | 63.73     | 47.29        | 58.22           | 52.57      |
| bert-cls-whitening                                  | 21.54     | 32.11     | 21.28     | 37.89     | 44.24     | 20.29        | 42.42           | 31.40      |

åœ¨Transferæ•°æ®é›†ä¸Šæ•ˆæœ

| Model                                              | MR        | CR        | SUBJ      | MPQA      | SST       | TREC      | MRPC  | Avg.      |
| -------------------------------------------------- | --------- | --------- | --------- | --------- | --------- | --------- | ----- | --------- |
| simcse-cpm-1b                                      | 85.10     | 90.28     | 94.43     | 90.53     | 90.01     | 91.20     | 76.46 | 88.29     |
| simcse-cpm-10b                                     | **89.22** | 92.21     | **95.12** | **91.34** | **93.47** | 91.20     | 75.13 | **89.67** |
| avg-bert                                           | 78.66     | 86.25     | 94.37     | 88.66     | 84.40     | **92.80** | 69.54 | 84.94     |
| SimCSE-RoBERT large                                | 88.12     | **92.37** | 95.11     | 90.49     | 92.75     | 91.80     | 76.64 | 89.61     |
| [m3ebase](https://huggingface.co/moka-ai/m3e-base) | 71.67     | 80.55     | 88.02     | 81.56     | 72.27     | 85.40     | 70.84 | 78.62     |
| GPT-embedding-002                                  |           |           |           |           |           |           |       |           |

Chinese STS-Benchmark

| æ¨¡å‹                      | Chinese-STS-B-dev | Chinese-STS-B-test |
| ------------------------- | ----------------- | ------------------ |
| bert_avg                  | 0.2549            | 0.2059             |
| sup_simcse_robertï¼ˆSNLIï¼‰ | 0.7499            | 0.6909             |
| simcse-cpm-1b             | 0.838             | 0.7743             |
| **simcse-cpm-10b**        | **0.836**         | **0.7936**         |
| m3e-base                  | 0.8245            | 0.7753             |



## å¼•ç”¨

```latex
@misc{sentCPM,
  author = {},
  title = {},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {}
}
```







